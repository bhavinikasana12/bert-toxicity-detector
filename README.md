# üõ°Ô∏è BERT Toxicity Classifier  

This project fine-tunes **BERT** to detect toxic comments. It classifies text as **toxic** or **non-toxic** using Natural Language Processing (NLP).  

## Features  
Fine-tuned **BERT-base-uncased** model  
Detects **toxic language** in text  
Trained on **Jigsaw Unintended Bias in Toxicity Classification** dataset  

## üöÄ Quick Start  

### **1Ô∏è‚É£ Clone the Repository**  
```bash
git clone https://github.com/yourusername/bert-toxicity-detector.git
cd bert-toxicity-detector
```
### **2Ô∏è‚É£ Install Dependencies
```bash
pip install -r requirements.txt
```
### **3Ô∏è‚É£ Run the Model
```bash
python main.py
```
